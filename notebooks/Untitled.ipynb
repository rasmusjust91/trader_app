{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_DATA_PATH = '../data_harvester/resources/data/reddit'\n",
    "TWITTER_DATA_PATH = '../data_harvester/resources/data/twitter'\n",
    "\n",
    "SHARED_COLUMNS = ['id', 'created', 'text', 'where']\n",
    "\n",
    "WORD2VEC_PATH = '../data_harvester/resources/models/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(files_dir, sep='|'):\n",
    "    \n",
    "    files = (pd.read_csv(os.path.join(files_dir ,f), sep=sep) for f in os.listdir(files_dir) if f.endswith('.csv'))\n",
    "    df = pd.concat(files, ignore_index=True)\n",
    "    \n",
    "    return df.drop_duplicates()#.reset_index(drop=True)\n",
    "\n",
    "def combine_twitter_and_reddit_data(twitter_data, reddit_data):\n",
    "    \n",
    "    twitter_data = twitter_data[['id_str', 'created_at', 'full_text', 'user.screen_name']]\n",
    "    twitter_data.columns=SHARED_COLUMNS\n",
    "    \n",
    "    reddit_data = reddit_data[['id', 'created', 'title', 'subreddit']]\n",
    "    reddit_data.columns=SHARED_COLUMNS\n",
    "    \n",
    "    combined = pd.concat((twitter_data, reddit_data), ignore_index=True)\n",
    "    \n",
    "    combined['date'] = pd.to_datetime(combined['created'], utc=True).dt.date\n",
    "    combined['time'] = pd.to_datetime(combined['created'], utc=True).dt.time\n",
    "    \n",
    "    return combined.drop('created', axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = load_csv_files(REDDIT_DATA_PATH)\n",
    "twitter_data = load_csv_files(TWITTER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_twitter_and_reddit_data(twitter_data, reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    WORD2VEC_PATH, \n",
    "    binary = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_traslator = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def text2vec(text, vec_len=300):\n",
    "    text = text.lower()\n",
    "    text = text.translate(punctuation_traslator)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    filtered_sentence = [w for w in text if not w in stop_words]\n",
    "    i = 1\n",
    "    vector_representation = np.zeros((1,vec_len))\n",
    "\n",
    "    for word in filtered_sentence:\n",
    "        try: \n",
    "            vector_representation = vector_representation + word2vec_model.wv[word]\n",
    "            i = i + 1\n",
    "        except KeyError:\n",
    "            i = i\n",
    "    vector_representation = np.divide(vector_representation, i)\n",
    "    return(vector_representation[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajuspandora.net/Documents/pandora-databricks/data_insights/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def generate_daily_text_features(data):\n",
    "    # TODO: Optimize this, its slow!\n",
    "    data['textvec'] = data['text'].apply(text2vec)\n",
    "    text_features = pd.DataFrame(data['textvec'].to_list(), columns=[f'textvec_{i}' for i in range(300)])\n",
    "    text_features['date'] = data['date']\n",
    "    return text_features.groupby('date').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text2vec('This is going bad')\n",
    "text2 = text2vec('This is going great')\n",
    "np.dot(text1, text2)/(np.linalg.norm(text1)*np.linalg.norm(text2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python37364bitvenvvenvdd3b089e424a457ea02d3d9a42871d2b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
